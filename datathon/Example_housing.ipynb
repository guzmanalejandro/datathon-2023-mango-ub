{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ronald Ortiz y Alejandro Guzman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento con el Dataset de Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z4/3bc0gxqx0zxdnxr2xbn_7py80000gn/T/ipykernel_73485/1804143763.py:2: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# some imports\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "plt.rc('font', size=12) \n",
    "plt.rc('figure', figsize = (12, 5))\n",
    "\n",
    "# Settings for the visualizations\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2,'font.family': [u'times']})\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 25)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "# create output folder\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "if not os.path.exists('output/session1'):\n",
    "    os.makedirs('output/session1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv('dataset/train_set.csv',index_col=0) \n",
    "\n",
    "price_cat = pd.cut(housing[\"Price\"],\n",
    "                               bins=[0., 500000, 1000000, 1500000, 2000000., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "housing['Postcode'] = pd.Categorical(housing.Postcode)\n",
    "housing['Postcode'] = housing['Postcode'].astype(str)\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, price_cat):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "    \n",
    "housing = strat_train_set.drop(\"Price\", axis=1) # drop labels for training set\n",
    "housing_num = housing.select_dtypes(include=[np.number])\n",
    "housing_labels = strat_train_set[\"Price\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "values = []\n",
    "# column index\n",
    "Rooms_ix, Bedroom2_ix, Bathroom_ix, BuildingArea_ix = 0, 2, 3, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        rooms_per_building_area = X[:, Rooms_ix] / (1.0 +X[:, BuildingArea_ix])# add 1 to avoid 0 division\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, Bedroom2_ix] / (1.0 + X[:, Bathroom_ix]) # add 1 to avoid 0 division\n",
    "            return np.c_[X, rooms_per_building_area, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_building_area]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "## create a function to replace 0 by NaN\n",
    "def replace_0_2_NaN(data):\n",
    "    data[data == 0] = np.nan\n",
    "    return data\n",
    "\n",
    "\n",
    "num0_pipeline = Pipeline([\n",
    "        ('zeros2NaN',FunctionTransformer(func = replace_0_2_NaN,validate=False)),\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('log',FunctionTransformer(np.log1p, validate=True)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        #('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('imputer', IterativeImputer(random_state=42)),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "from category_encoders import TargetEncoder, CatBoostEncoder, JamesSteinEncoder\n",
    "\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"constant\", fill_value='Unknown')),\n",
    "        ('cat_boost_encoder', CatBoostEncoder()),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs0 = ['Landsize','BuildingArea']\n",
    "num_attribs1 = ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "cat_attribs = ['Type', 'Regionname', 'CouncilArea', 'Suburb','Postcode','SellerG']\n",
    "\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num0\", num0_pipeline, num_attribs0),\n",
    "        (\"num1\", num_pipeline, num_attribs1),\n",
    "        (\"cat\", cat_pipeline, cat_attribs),\n",
    "    ])\n",
    "housing_prepared = full_pipeline.fit_transform(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "housing_prepared.shape\n",
    "drop_columns2 = []\n",
    "columns2 = []\n",
    "for i in range(housing_prepared.shape[1]):\n",
    "    data1 = housing_prepared[:,i]\n",
    "    data2 = housing_labels.values\n",
    "    mean1 = data1.mean() \n",
    "    mean2 = data2.mean()\n",
    "    std1 = data1.std()\n",
    "    std2 = data2.std()\n",
    "    corr = ((data1*data2).mean()-mean1*mean2)/(std1*std2)\n",
    "    if abs(corr) < 0.1: # 0.1 es el punto de inflexión aprox\n",
    "        drop_columns2.append(i)\n",
    "    else:\n",
    "        columns2.append(i)\n",
    "housing_prepared =np.delete(housing_prepared, drop_columns2, axis=1)  #Comentar si no se quiere eliminar datos con poca correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4345x18 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 78210 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_prepared = csr_matrix(housing_prepared)\n",
    "housing_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': False, 'max_features': 6, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 110}\n"
     ]
    }
   ],
   "source": [
    "## Let's try another model: Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # try 12 (3×4) combinations of hyperparameters\n",
    "    #{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8], 'max_depth':[3,5,7,10]},\n",
    "    # then try 6 (2×3) combinations with bootstrap set as False\n",
    "    {'bootstrap': [False], 'n_estimators': [110], 'max_features': [6,7], 'min_samples_split': [3,5], 'min_samples_leaf': [1]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_root_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"Price\", axis=1)\n",
    "y_test = strat_test_set[\"Price\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "X_test_prepared = np.delete(X_test_prepared, drop_columns2, axis=1) #Comentar si no se quiere eliminar datos con poca correlación\n",
    "X_test_prepared = csr_matrix(X_test_prepared)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = full_pipeline.fit(housing, housing_labels)\n",
    "housing_prepared = full_pipeline.transform(housing)\n",
    "X_test_prepared = full_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290821.4477420408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([257409.41406214, 320771.91661014])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(final_rmse)\n",
    "\n",
    "from scipy import stats\n",
    "confidence = 0.95\n",
    "squared_errors = (final_predictions - y_test) ** 2\n",
    "np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
    "                         loc=squared_errors.mean(),\n",
    "                         scale=stats.sem(squared_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba con el Dataset de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv('dataset/train_set.csv',index_col=0) \n",
    "test = pd.read_csv('dataset/test_set.csv/test_set.csv',index_col=0) \n",
    "\n",
    "housing['Postcode'] = pd.Categorical(housing.Postcode)\n",
    "housing['Postcode'] = housing['Postcode'].astype(str)\n",
    "test['Postcode'] = pd.Categorical(test.Postcode)\n",
    "test['Postcode'] = test['Postcode'].astype(str)\n",
    "\n",
    "housing_labels = housing[\"Price\"].copy()\n",
    "housing = housing.drop(\"Price\", axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/impute/_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#cat_attribs = ['Type', 'CouncilArea', 'Suburb', 'Postcode']\n",
    "cat_attribs = ['Type', 'Regionname', 'CouncilArea', 'Suburb','Postcode','SellerG']\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num0\", num0_pipeline, num_attribs0),\n",
    "        (\"num1\", num_pipeline, num_attribs1),\n",
    "        (\"cat\", cat_pipeline, cat_attribs),\n",
    "    ])\n",
    "full_pipeline = full_pipeline.fit(housing, housing_labels)\n",
    "housing_prepared = full_pipeline.transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = []\n",
    "columns = []\n",
    "for i in range(housing_prepared.shape[1]):\n",
    "    data1 = housing_prepared[:,i]\n",
    "    data2 = housing_labels.values\n",
    "    mean1 = np.mean(data1) \n",
    "    mean2 = np.mean(data2)\n",
    "    std1 = np.std(data1)\n",
    "    std2 = np.std(data2)\n",
    "    corr = ((data1*data2).mean()-mean1*mean2)/(std1*std2)\n",
    "    if abs(corr) < 0.1:\n",
    "        drop_columns.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared =np.delete(housing_prepared, drop_columns, axis=1)\n",
    "housing_prepared = csr_matrix(housing_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'bootstrap': [False], 'n_estimators': [125], 'max_features': [4], 'min_samples_split': [2], 'min_samples_leaf': [2]},\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': False, 'max_features': 4, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 125}\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_root_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prepared = full_pipeline.transform(test)\n",
    "test_prepared = np.delete(test_prepared, drop_columns, axis=1) #Comentar si no se quiere eliminar datos con poca correlación\n",
    "test_prepared = csr_matrix(test_prepared)\n",
    "final_model = grid_search.best_estimator_\n",
    "y_pred = final_model.predict(test_prepared)\n",
    "\n",
    "df_output = pd.DataFrame(y_pred)\n",
    "df_output = df_output.reset_index()\n",
    "df_output.columns = ['index','Price']\n",
    "\n",
    "df_output.to_csv('output/session1/baseline.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación de los cambios realizados:\n",
    "\n",
    "En primera parte se ha realizado diversas mejoras en el postprocesamiento de los datos:\n",
    "\n",
    "- **CatBoost Encoder**: Se ha definido un CatBoostEncoder para realizar la codificación basada en el objetivo de las variables categóricas. La clase realiza una codificación consciente del tiempo, con regularización y aprendizaje en línea, de cada categoría única en la matriz de entrada X, y reemplaza cada categoría con un valor calculado basado en el objetivo. Finalmente, se eliminan las columnas originales. Este enfoque mejora el one-hot encoder estándar al reducir la dimensionalidad del dataset y al permitir la captura de relaciones entre las categorías y la variable objetivo, especialmente en datasets con un gran número de categorías únicas.\n",
    "\n",
    "- **Columnas poco correlacionadas**: Se ha añadido un segmento de código que calcula el coeficiente de correlación de Pearson entre cada columna de housing_prepared y housing_labels, identificando las columnas con una correlación débil (menor a 0.1) para posiblemente eliminarlas. Este enfoque puede mejorar la eficiencia del modelo al reducir la dimensionalidad y eliminar características no informativas. En la práctica hemos observado que ha mejorado bastante el rendimiento.\n",
    "\n",
    "- **Iterative Imputer en variables numéricas**: El Iterative Imputer es generalmente mejor que el Single Imputer para variables numéricas porque, en lugar de imputar valores faltantes utilizando una estadística simple como la media o la mediana, utiliza modelos predictivos para estimar los valores faltantes considerando las relaciones entre todas las variables. Esto puede resultar en imputaciones más precisas y realistas.\n",
    "\n",
    "Finalmente se ha realizado un reajuste de los hiperparámetros del modelo de Random Forest para obtener un mayor rendimiento:\n",
    "\n",
    "- En primer lugar se han probado varios hiperparámetros tanto para Bootstrap=False como Boostrap=True pero en el 100% de los casos la función GridSearchCV ha encontrado un menor error cuadrático con el Bootstrap=False.\n",
    "- Los mejores hiperparámetros encontrados han sido:\n",
    "\n",
    "    **One Hot Encoder**: {'bootstrap': [False], 'n_estimators': [125], 'max_features': [10], 'min_samples_split': [4], 'min_samples_leaf': [2]}\n",
    "    \n",
    "    **Cat Boost Encoder**: {'bootstrap': [False], 'n_estimators': [125], 'max_features': [4], 'min_samples_split': [2], 'min_samples_leaf': [2]}\n",
    "\n",
    "**PUNTUACIÓN FINAL** Implementando Cat Boost Encoder (297500) se ha obtenido un mejor private score en el Kaggle y con One Hot Encoder se ha obtenido un mejor public score (279111).\n",
    "\n",
    "- *Puntuación media One Hot Encoder*: 290.000\n",
    "- *Puntuación media CatBoost Encoder*: 289.500"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
